{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from itertools import chain\n",
    "import math\n",
    "import twittergraph as tg\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 181416 tweets\n"
     ]
    }
   ],
   "source": [
    "graph = tg.LoadTwitterGraph('/Users/tomfw/Downloads/DataShared/', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_train = graph.copy()\n",
    "g_test = graph.copy()\n",
    "\n",
    "for u, v in g_train.edges():\n",
    "    if random.random() < .2:\n",
    "        g_train.remove_edge(u, v)\n",
    "\n",
    "for u, v in g_test.edges():\n",
    "    if random.random() < .2:\n",
    "        g_test.remove_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing training features...\n",
      "0 in set so far...\n",
      "50000 in set so far...\n",
      "100000 in set so far...\n",
      "150000 in set so far...\n",
      "200000 in set so far...\n",
      "250000 in set so far...\n",
      "300000 in set so far...\n",
      "311310 pairs and 0 edges in dataframe\n",
      "Computing testing features...\n",
      "0 in set so far...\n",
      "50000 in set so far...\n",
      "100000 in set so far...\n",
      "150000 in set so far...\n",
      "200000 in set so far...\n",
      "250000 in set so far...\n",
      "300000 in set so far...\n",
      "310036 pairs and 0 edges in dataframe\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing training features...\")\n",
    "df_train = tg.dataframe_from_graph(g_train, sampling=.5, pairs=False)\n",
    "labels_train = tg.labels_for_dataframe(df_train, graph)\n",
    "print(\"Computing testing features...\")\n",
    "df_test = tg.dataframe_from_graph(g_test, sampling=.5, pairs=False)\n",
    "labels_test = tg.labels_for_dataframe(df_test, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7831 actual edges in test set\n"
     ]
    }
   ],
   "source": [
    "print(\"%d actual edges in test set\" % np.sum(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_depth=None,\n",
    "   min_samples_split=2, random_state=0, )\n",
    "# rf = LinearSVC()\n",
    "fields = ['adam', 'jac', 'spl', 'nbrs', 'att']\n",
    "x_train = df_train.loc[:, fields]\n",
    "y_train = labels_train\n",
    "\n",
    "x_test = df_test.loc[:, fields]\n",
    "classifier = rf.fit(x_train, y_train)\n",
    "pred = classifier.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_pred = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i, 1] > .35:\n",
    "        bin_pred.append(True)\n",
    "    else:\n",
    "        bin_pred.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770259428555\n",
      "0.565606108853\n",
      "3699\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(labels_test, pred[:, 1]))\n",
    "print(roc_auc_score(labels_test, bin_pred))\n",
    "print(np.sum(bin_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6370\n",
      "Recall: 0.5656\n",
      "F-Score: 0.5873\n"
     ]
    }
   ],
   "source": [
    "(pr, re, fs, su) = precision_recall_fscore_support(labels_test, bin_pred, average='macro')\n",
    "print(\"Precision: %.4f\" % pr)\n",
    "print(\"Recall: %.4f\" % re)\n",
    "print(\"F-Score: %.4f\" % fs)\n",
    "# print(\"Support: %.4f\" % su)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUC is sometimes good this way when using probabilities\n",
    "# AUC can be good with binary predictions with significant adjustment of the threshold\n",
    "# Adjusting too much to achieve higher AUC results in lower F-Measure\n",
    "# Also when using binary F-Measure is reported to be < .3, with macro ~.6, with micro > .95\n",
    "# Pretty sure this is because of the significant imbalance of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 300696\n",
      "Incorrect predictions: 9340\n",
      "\n",
      "1095 true positive\n",
      "6736 false negative\n",
      "2604 false positives\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "correct_edges = 0\n",
    "incorrect_edges = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "for i in range(0,df_test.shape[0]):\n",
    "    prediction = bin_pred[i]\n",
    "    actu = labels_test[i]\n",
    "    if prediction == actu:\n",
    "        correct += 1\n",
    "        if actu:\n",
    "            correct_edges += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        if actu:\n",
    "            incorrect_edges += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "\n",
    "print(\"Correct predictions: %d\" % correct)\n",
    "print(\"Incorrect predictions: %d\\n\" % incorrect)\n",
    "\n",
    "print(\"%d true positive\" % correct_edges)\n",
    "print(\"%d false negative\" % incorrect_edges)\n",
    "print(\"%d false positives\" % false_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}